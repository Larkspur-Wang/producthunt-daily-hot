import os
# from dotenv import load_dotenv
import requests
from datetime import datetime, timedelta, timezone
from bs4 import BeautifulSoup
import pytz
import time
import random
import json
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from google import genai

# åŠ è½½ .env æ–‡ä»¶
# load_dotenv()

GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')
producthunt_client_id = os.getenv('PRODUCTHUNT_CLIENT_ID')
producthunt_client_secret = os.getenv('PRODUCTHUNT_CLIENT_SECRET')

def call_gemini_api(prompt: str, model: str = "gemini-2.0-flash-exp", temperature: float = 0.7) -> str:
    """è°ƒç”¨Google Gemini API"""
    try:
        if not GOOGLE_API_KEY:
            raise ValueError("GOOGLE_API_KEY not found in environment variables")
            
        genai.configure(api_key=GOOGLE_API_KEY)
        client = genai.GenerativeModel(model)
        response = client.generate_content(
            prompt,
            generation_config=genai.types.GenerationConfig(
                temperature=temperature,
                top_k=40,
                top_p=0.95,
            )
        )
        
        if response and response.text:
            return response.text.strip()
        else:
            return ""
    except Exception as e:
        print(f"APIè°ƒç”¨é”™è¯¯: {e}")
        return ""

class Product:
    def __init__(self, id: str, name: str, tagline: str, description: str, votesCount: int, createdAt: str, featuredAt: str, website: str, url: str, **kwargs):
        self.name = name
        self.tagline = tagline
        self.description = description
        self.votes_count = votesCount
        self.created_at = self.convert_to_beijing_time(createdAt)
        self.featured = "æ˜¯" if featuredAt else "å¦"
        self.website = website
        self.url = url
        self.og_image_url = self.fetch_og_image_url()
        self.keyword = self.generate_keywords()
        self.translated_tagline = self.translate_text(self.tagline)
        self.translated_description = self.translate_text(self.description)

    def fetch_og_image_url(self) -> str:
        """è·å–äº§å“çš„Open Graphå›¾ç‰‡URL"""
        print(f"æ­£åœ¨è·å–URL: {self.url}")  # æ‰“å°æ­£åœ¨è¯·æ±‚çš„URL
        
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3"
        }
        response = requests.get(self.url, headers=headers)
        print(f"è¯·æ±‚çŠ¶æ€ç : {response.status_code}")  # æ‰“å°è¯·æ±‚çŠ¶æ€ç 
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            # æŸ¥æ‰¾og:image metaæ ‡ç­¾
            og_image = soup.find("meta", property="og:image")
            if og_image:
                print(f"æ‰¾åˆ° og:image æ ‡ç­¾: {og_image}") # æ‰“å° og:image æ ‡ç­¾å†…å®¹
                return og_image["content"]
            # å¤‡ç”¨:æŸ¥æ‰¾twitter:image metaæ ‡ç­¾
            twitter_image = soup.find("meta", name="twitter:image")
            if twitter_image:
                print(f"æ‰¾åˆ° twitter:image æ ‡ç­¾: {twitter_image}") # æ‰“å° twitter:image æ ‡ç­¾å†…å®¹
                return twitter_image["content"]
        print("æ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡ URL") # æ‰“å°æœªæ‰¾åˆ°å›¾ç‰‡ URL çš„æ¶ˆæ¯
        return ""

    def generate_keywords(self) -> str:
        """ç”Ÿæˆäº§å“çš„å…³é”®è¯ï¼Œæ˜¾ç¤ºåœ¨ä¸€è¡Œï¼Œç”¨é€—å·åˆ†éš”"""
        prompt = f"æ ¹æ®ä»¥ä¸‹å†…å®¹ç”Ÿæˆ3ä¸ªé€‚åˆçš„ä¸­æ–‡å…³é”®è¯ï¼Œç”¨è‹±æ–‡é€—å·åˆ†éš”å¼€ï¼š\n\näº§å“åç§°ï¼š{self.name}\n\næ ‡è¯­ï¼š{self.tagline}\n\næè¿°ï¼š{self.description}"
        
        max_retries = 3
        for attempt in range(max_retries):
            try:
                keywords = call_gemini_api(prompt, model="gemini-2.0-flash-exp")
                if keywords:
                    if ',' not in keywords:
                        keywords = ', '.join(keywords.split())
                    time.sleep(random.uniform(5, 8))  # éšæœºç­‰å¾…5-8ç§’
                    return keywords
                else:
                    print(f"APIè¿”å›ç©ºå“åº”ï¼Œå°è¯•é‡è¯• {attempt + 1}/{max_retries}")
            except Exception as e:
                print(f"ç”Ÿæˆå…³é”®è¯æ—¶å‘ç”Ÿé”™è¯¯ï¼ˆå°è¯• {attempt + 1}/{max_retries}ï¼‰: {e}")
                if attempt < max_retries - 1:
                    time.sleep(random.uniform(8, 12))  # åœ¨é‡è¯•ä¹‹å‰éšæœºç­‰å¾…8-12ç§’
                else:
                    return "æ— å…³é”®è¯"
        
        return "æ— å…³é”®è¯"

    def translate_text(self, text: str) -> str:
        """ä½¿ç”¨Google Geminiç¿»è¯‘æ–‡æœ¬å†…å®¹"""
        prompt = f"å°†ä»¥ä¸‹è‹±æ–‡ç¿»è¯‘æˆåœ°é“çš„ä¸­æ–‡ï¼Œé£æ ¼ä¸ç§‘æ™®æ‚å¿—æˆ–æ—¥å¸¸å¯¹è¯ç›¸ä¼¼ã€‚ä¸è¦æœ‰ç¿»è¯‘å¤–çš„é¢å¤–å†…å®¹ï¼š\n\n{text}"
        
        max_retries = 3
        for attempt in range(max_retries):
            try:
                translated_text = call_gemini_api(prompt, model="gemini-2.0-flash-exp")
                if translated_text:
                    time.sleep(5)  # åœ¨APIè°ƒç”¨åç­‰å¾…5ç§’
                    return translated_text
                else:
                    print(f"APIè¿”å›ç©ºå“åº”ï¼Œå°è¯•é‡è¯• {attempt + 1}/{max_retries}")
            except Exception as e:
                print(f"ç¿»è¯‘æ–‡æœ¬æ—¶å‘ç”Ÿé”™è¯¯ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(random.uniform(8, 12))  # åœ¨é‡è¯•ä¹‹å‰éšæœºç­‰å¾…8-12ç§’
                else:
                    return text  # æœ€åä¸€æ¬¡é‡è¯•å¤±è´¥ï¼Œè¿”å›åŸå§‹æ–‡æœ¬
        
        return text

    def convert_to_beijing_time(self, utc_time_str: str) -> str:
        """å°†UTCæ—¶é—´è½¬æ¢ä¸ºåŒ—äº¬æ—¶é—´"""
        utc_time = datetime.strptime(utc_time_str, '%Y-%m-%dT%H:%M:%SZ')
        beijing_tz = pytz.timezone('Asia/Shanghai')
        beijing_time = utc_time.replace(tzinfo=pytz.utc).astimezone(beijing_tz)
        return beijing_time.strftime('%Yå¹´%mæœˆ%dæ—¥ %p%I:%M (åŒ—äº¬æ—¶é—´)')

    def to_markdown(self, rank: int) -> str:
        """è¿”å›äº§å“æ•°æ®çš„Markdownæ ¼å¼"""
        og_image_markdown = f"![{self.name}]({self.og_image_url})"
        return (
            f"## [{rank}. {self.name}]({self.url})  \n"
            f"{og_image_markdown}  \n\n"
            f"**æ ‡è¯­**ï¼š{self.translated_tagline}  \n"
            f"**ä»‹ç»**ï¼š{self.translated_description}  \n"
            f"**ç¥¨æ•°**: ğŸ”º{self.votes_count}  \n"
            f"**å…³é”®è¯**ï¼š{self.keyword}  \n"
            f"**å‘å¸ƒæ—¶é—´**ï¼š{self.created_at}  \n\n"
            #f"**äº§å“ç½‘ç«™**: [ç«‹å³è®¿é—®]({self.website})  \n"
            #f"**Product Hunt**: [View on Product Hunt]({self.url})\n\n"                                  
            #f"**æ˜¯å¦ç²¾é€‰**ï¼š{self.featured} \n"           
            f"---\n\n"
        )

def get_producthunt_token():
    """ä½¿ç”¨ developer token è¿›è¡Œè®¤è¯"""
    token = os.getenv('PRODUCTHUNT_DEVELOPER_TOKEN')
    print(f"token is {token};")
    if not token:
        raise Exception("Product Hunt developer token not found in environment variables")
    return token

def fetch_product_hunt_data():
    """ä»Product Huntè·å–å‰ä¸€å¤©çš„Top 24æ•°æ®"""
    token = get_producthunt_token()
    yesterday = datetime.now(timezone.utc) - timedelta(days=1)
    date_str = yesterday.strftime('%Y-%m-%d')
    url = "https://api.producthunt.com/v2/api/graphql"
    
    # æ·»åŠ æ›´å¤šè¯·æ±‚å¤´ä¿¡æ¯
    headers = {
        "Accept": "application/json",
        "Content-Type": "application/json",
        "Authorization": f"Bearer {token}",
        "User-Agent": "DecohackBot/1.0 (https://decohack.com)",
        "Origin": "https://decohack.com",
        "Accept-Language": "en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7",
        "Connection": "keep-alive"
    }
    
    base_query = """
    {
      posts(order: VOTES, postedAfter: "%sT00:00:00Z", postedBefore: "%sT23:59:59Z", after: "%s") {
        nodes {
          id
          name
          tagline
          description
          votesCount
          createdAt
          featuredAt
          website
          url
        }
        pageInfo {
          hasNextPage
          endCursor
        }
      }
    }
    """

    all_posts = []
    has_next_page = True
    cursor = ""

    while has_next_page and len(all_posts) < 24:
        query = base_query % (date_str, date_str, cursor)
        response = requests.post(url, headers=headers, json={"query": query})

        if response.status_code != 200:
            raise Exception(f"Failed to fetch data from Product Hunt: {response.status_code}, {response.text}")

        data = response.json()['data']['posts']
        posts = data['nodes']
        all_posts.extend(posts)

        has_next_page = data['pageInfo']['hasNextPage']
        cursor = data['pageInfo']['endCursor']

    # åªä¿ç•™å‰24ä¸ªäº§å“
    return [Product(**post) for post in sorted(all_posts, key=lambda x: x['votesCount'], reverse=True)[:24]]

def generate_markdown(products, date_str):
    """ç”ŸæˆMarkdownå†…å®¹å¹¶ä¿å­˜åˆ°dataç›®å½•"""
    # è·å–ä»Šå¤©çš„æ—¥æœŸå¹¶æ ¼å¼åŒ–
    today = datetime.now(timezone.utc)
    date_today = today.strftime('%Y-%m-%d')

    markdown_content = f"# PHä»Šæ—¥çƒ­æ¦œ | {date_today}\n\n"
    for rank, product in enumerate(products, 1):
        markdown_content += product.to_markdown(rank)

    # ç¡®ä¿ data ç›®å½•å­˜åœ¨
    os.makedirs('data', exist_ok=True)

    # ä¿®æ”¹æ–‡ä»¶ä¿å­˜è·¯å¾„åˆ° data ç›®å½•
    file_name = f"data/producthunt-daily-{date_today}.md"
    
    # å¦‚æœæ–‡ä»¶å­˜åœ¨ï¼Œç›´æ¥è¦†ç›–
    with open(file_name, 'w', encoding='utf-8') as file:
        file.write(markdown_content)
    print(f"æ–‡ä»¶ {file_name} ç”ŸæˆæˆåŠŸå¹¶å·²è¦†ç›–ã€‚")


def main():
    # è·å–æ˜¨å¤©çš„æ—¥æœŸå¹¶æ ¼å¼åŒ–
    yesterday = datetime.now(timezone.utc) - timedelta(days=1)
    date_str = yesterday.strftime('%Y-%m-%d')

    # è·å–Product Huntæ•°æ®
    products = fetch_product_hunt_data()

    # ç”Ÿæˆå…³é”®è¯å’Œç¿»è¯‘
    for product in products:
        product.keyword = product.generate_keywords()
        product.translated_tagline = product.translate_text(product.tagline)
        product.translated_description = product.translate_text(product.description)
        time.sleep(2)  # åœ¨æ¯ä¸ªäº§å“çš„å¤„ç†ä¹‹é—´æ·»åŠ 2ç§’çš„å»¶è¿Ÿ

    # ç”ŸæˆMarkdownæ–‡ä»¶
    generate_markdown(products, date_str)

if __name__ == "__main__":
    main()
