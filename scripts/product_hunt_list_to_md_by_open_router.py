import os
# from dotenv import load_dotenv
import requests
from datetime import datetime, timedelta, timezone
from openai import OpenAI
from bs4 import BeautifulSoup
import pytz
import time
import random
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# åŠ è½½ .env æ–‡ä»¶
# load_dotenv()

# åˆ›å»º OpenAI å®¢æˆ·ç«¯å®ä¾‹
client = OpenAI(
    base_url = "https://openrouter.ai/api/v1",
    api_key=os.getenv('OPENAI_API_KEY'),
    )

producthunt_client_id = os.getenv('PRODUCTHUNT_CLIENT_ID')
producthunt_client_secret = os.getenv('PRODUCTHUNT_CLIENT_SECRET')

class Product:
    def __init__(self, id: str, name: str, tagline: str, description: str, votesCount: int, createdAt: str, featuredAt: str, website: str, url: str, **kwargs):
        self.name = name
        self.tagline = tagline
        self.description = description
        self.votes_count = votesCount
        self.created_at = self.convert_to_beijing_time(createdAt)
        self.featured = "æ˜¯" if featuredAt else "å¦"
        self.website = website
        self.url = url
        self.og_image_url = self.fetch_og_image_url()
        self.keyword = self.generate_keywords()
        self.translated_tagline = self.translate_text(self.tagline)
        self.translated_description = self.translate_text(self.description)

   def fetch_og_image_url(self) -> str:
        """è·å–äº§å“çš„Open Graphå›¾ç‰‡URL"""
        response = requests.get(self.url)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            # æŸ¥æ‰¾og:image metaæ ‡ç­¾
            og_image = soup.find("meta", property="og:image")
            if og_image:
                return og_image["content"]
            # å¤‡ç”¨:æŸ¥æ‰¾twitter:image metaæ ‡ç­¾
            twitter_image = soup.find("meta", name="twitter:image") 
            if twitter_image:
                return twitter_image["content"]
        return ""

    def generate_keywords(self) -> str:
        """ç”Ÿæˆäº§å“çš„å…³é”®è¯ï¼Œæ˜¾ç¤ºåœ¨ä¸€è¡Œï¼Œç”¨é€—å·åˆ†éš”"""
        prompt = f"æ ¹æ®ä»¥ä¸‹å†…å®¹ç”Ÿæˆ3ä¸ªé€‚åˆçš„ä¸­æ–‡å…³é”®è¯ï¼Œç”¨è‹±æ–‡é€—å·åˆ†éš”å¼€ï¼š\n\näº§å“åç§°ï¼š{self.name}\n\næ ‡è¯­ï¼š{self.tagline}\n\næè¿°ï¼š{self.description}"
        
        max_retries = 3
        for attempt in range(max_retries):
            try:
                response = client.chat.completions.create(
                    model="meta-llama/llama-3.1-405b-instruct:free",
                    messages=[
                        {"role": "system", "content": "Generate suitable Chinese keywords based on the product information provided. The keywords should be separated by commas."},
                        {"role": "user", "content": prompt},
                    ],
                    max_tokens=50,
                    temperature=0.7,
                )
                if response.choices and response.choices[0].message:
                    keywords = response.choices[0].message.content.strip()
                    if ',' not in keywords:
                        keywords = ', '.join(keywords.split())
                    time.sleep(random.uniform(3, 5))  # éšæœºç­‰å¾…3-5ç§’
                    return keywords
                else:
                    print(f"APIè¿”å›ç©ºå“åº”ï¼Œå°è¯•é‡è¯• {attempt + 1}/{max_retries}")
            except Exception as e:
                print(f"ç”Ÿæˆå…³é”®è¯æ—¶å‘ç”Ÿé”™è¯¯ï¼ˆå°è¯• {attempt + 1}/{max_retries}ï¼‰: {e}")
                if attempt < max_retries - 1:
                    time.sleep(random.uniform(5, 10))  # åœ¨é‡è¯•ä¹‹å‰éšæœºç­‰å¾…5-10ç§’
                else:
                    return "æ— å…³é”®è¯"
        
        return "æ— å…³é”®è¯"

    def translate_text(self, text: str) -> str:
        """ä½¿ç”¨OpenAIç¿»è¯‘æ–‡æœ¬å†…å®¹"""
        try:
            response = client.chat.completions.create(
                model="nousresearch/hermes-3-llama-3.1-405b:free",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸–ç•Œä¸Šæœ€ä¸“ä¸šçš„ç¿»è¯‘å·¥å…·ï¼Œæ“…é•¿è‹±æ–‡å’Œä¸­æ–‡äº’è¯‘ã€‚ä½ æ˜¯ä¸€ä½ç²¾é€šè‹±æ–‡å’Œä¸­æ–‡çš„ä¸“ä¸šç¿»è¯‘ï¼Œå°¤å…¶æ“…é•¿å°†ITå…¬å¸é»‘è¯å’Œä¸“ä¸šè¯æ±‡ç¿»è¯‘æˆç®€æ´æ˜“æ‡‚çš„åœ°é“è¡¨è¾¾ã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†ä»¥ä¸‹å†…å®¹ç¿»è¯‘æˆåœ°é“çš„ä¸­æ–‡ï¼Œé£æ ¼ä¸ç§‘æ™®æ‚å¿—æˆ–æ—¥å¸¸å¯¹è¯ç›¸ä¼¼ã€‚"},
                    {"role": "user", "content": text},
                ],
                max_tokens=500,
                temperature=0.7,
            )
            translated_text = response.choices[0].message.content.strip()
            time.sleep(3)  # åœ¨APIè°ƒç”¨åç­‰å¾…3ç§’
            return translated_text
        except Exception as e:
            print(f"Error occurred during translation: {e}")
            return text

    def convert_to_beijing_time(self, utc_time_str: str) -> str:
        """å°†UTCæ—¶é—´è½¬æ¢ä¸ºåŒ—äº¬æ—¶é—´"""
        utc_time = datetime.strptime(utc_time_str, '%Y-%m-%dT%H:%M:%SZ')
        beijing_tz = pytz.timezone('Asia/Shanghai')
        beijing_time = utc_time.replace(tzinfo=pytz.utc).astimezone(beijing_tz)
        return beijing_time.strftime('%Yå¹´%mæœˆ%dæ—¥ %p%I:%M (åŒ—äº¬æ—¶é—´)')

    def to_markdown(self, rank: int) -> str:
        """è¿”å›äº§å“æ•°æ®çš„Markdownæ ¼å¼"""
        og_image_markdown = f"![{self.name}]({self.og_image_url})"
        return (
            f"## [{rank}. {self.name}]({self.url})  \n"
            f"{og_image_markdown}  \n\n"
            f"**æ ‡è¯­**ï¼š{self.translated_tagline}  \n"
            f"**ä»‹ç»**ï¼š{self.translated_description}  \n"
            f"**ç¥¨æ•°**: ğŸ”º{self.votes_count}  \n"
            f"**å…³é”®è¯**ï¼š{self.keyword}  \n"
            f"**å‘å¸ƒæ—¶é—´**ï¼š{self.created_at}  \n\n"
            #f"**äº§å“ç½‘ç«™**: [ç«‹å³è®¿é—®]({self.website})  \n"
            #f"**Product Hunt**: [View on Product Hunt]({self.url})\n\n"                                  
            #f"**æ˜¯å¦ç²¾é€‰**ï¼š{self.featured} \n"           
            f"---\n\n"
        )

def get_producthunt_token():
    """ä½¿ç”¨ developer token è¿›è¡Œè®¤è¯"""
    token = os.getenv('PRODUCTHUNT_DEVELOPER_TOKEN')
    print(f"token is {token};")
    if not token:
        raise Exception("Product Hunt developer token not found in environment variables")
    return token

def fetch_product_hunt_data():
    """ä»Product Huntè·å–å‰ä¸€å¤©çš„Top 24æ•°æ®"""
    token = get_producthunt_token()
    yesterday = datetime.now(timezone.utc) - timedelta(days=1)
    date_str = yesterday.strftime('%Y-%m-%d')
    url = "https://api.producthunt.com/v2/api/graphql"
    
    # æ·»åŠ æ›´å¤šè¯·æ±‚å¤´ä¿¡æ¯
    headers = {
        "Accept": "application/json",
        "Content-Type": "application/json",
        "Authorization": f"Bearer {token}",
        "User-Agent": "DecohackBot/1.0 (https://decohack.com)",
        "Origin": "https://decohack.com",
        "Accept-Language": "en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7",
        "Connection": "keep-alive"
    }
    
    base_query = """
    {
      posts(order: VOTES, postedAfter: "%sT00:00:00Z", postedBefore: "%sT23:59:59Z", after: "%s") {
        nodes {
          id
          name
          tagline
          description
          votesCount
          createdAt
          featuredAt
          website
          url
        }
        pageInfo {
          hasNextPage
          endCursor
        }
      }
    }
    """

    all_posts = []
    has_next_page = True
    cursor = ""

    while has_next_page and len(all_posts) < 24:
        query = base_query % (date_str, date_str, cursor)
        response = requests.post(url, headers=headers, json={"query": query})

        if response.status_code != 200:
            raise Exception(f"Failed to fetch data from Product Hunt: {response.status_code}, {response.text}")

        data = response.json()['data']['posts']
        posts = data['nodes']
        all_posts.extend(posts)

        has_next_page = data['pageInfo']['hasNextPage']
        cursor = data['pageInfo']['endCursor']

    # åªä¿ç•™å‰24ä¸ªäº§å“
    return [Product(**post) for post in sorted(all_posts, key=lambda x: x['votesCount'], reverse=True)[:24]]

def generate_markdown(products, date_str):
    """ç”ŸæˆMarkdownå†…å®¹å¹¶ä¿å­˜åˆ°dataç›®å½•"""
    # è·å–ä»Šå¤©çš„æ—¥æœŸå¹¶æ ¼å¼åŒ–
    today = datetime.now(timezone.utc)
    date_today = today.strftime('%Y-%m-%d')

    markdown_content = f"# PHä»Šæ—¥çƒ­æ¦œ | {date_today}\n\n"
    for rank, product in enumerate(products, 1):
        markdown_content += product.to_markdown(rank)

    # ç¡®ä¿ data ç›®å½•å­˜åœ¨
    os.makedirs('data', exist_ok=True)

    # ä¿®æ”¹æ–‡ä»¶ä¿å­˜è·¯å¾„åˆ° data ç›®å½•
    file_name = f"data/producthunt-daily-{date_today}.md"
    
    # å¦‚æœæ–‡ä»¶å­˜åœ¨ï¼Œç›´æ¥è¦†ç›–
    with open(file_name, 'w', encoding='utf-8') as file:
        file.write(markdown_content)
    print(f"æ–‡ä»¶ {file_name} ç”ŸæˆæˆåŠŸå¹¶å·²è¦†ç›–ã€‚")


def main():
    # è·å–æ˜¨å¤©çš„æ—¥æœŸå¹¶æ ¼å¼åŒ–
    yesterday = datetime.now(timezone.utc) - timedelta(days=1)
    date_str = yesterday.strftime('%Y-%m-%d')

    # è·å–Product Huntæ•°æ®
    products = fetch_product_hunt_data()

    # ç”Ÿæˆå…³é”®è¯å’Œç¿»è¯‘
    for product in products:
        product.keyword = product.generate_keywords()
        product.translated_tagline = product.translate_text(product.tagline)
        product.translated_description = product.translate_text(product.description)
        time.sleep(2)  # åœ¨æ¯ä¸ªäº§å“çš„å¤„ç†ä¹‹é—´æ·»åŠ 2ç§’çš„å»¶è¿Ÿ

    # ç”ŸæˆMarkdownæ–‡ä»¶
    generate_markdown(products, date_str)

if __name__ == "__main__":
    main()
